# -*- coding: utf-8 -*-
"""SHAP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fWyLlGJGjJPsVNRuX59qUMQZi6uIuQ3t
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import os
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import shap

# Dataset that can be used to predict the chronic kidney diseas
df = pd.read_csv('kidney_disease.csv',sep=',', header=0)

df.head()

df=df.drop(['rbc'],axis=1)

df=df.drop(['sod'],axis=1)

df=df.drop(['wc'],axis=1)

df=df.drop(['pot'],axis=1)

df.isnull().sum()

df=df.dropna()

df.shape

X=df.iloc[:,1:25]

y = (df['classification'] == "ckd")

"""## Create dummy variables from categorical variables"""

X_dum=X[['pc','pcc','ba','htn','dm','cad','appet','pe','ane']]

X_dum=pd.get_dummies(X_dum)

X_dum.head()

result=pd.concat([X_dum,X[['age','bp','sg','al','su','bgr','bu','sc','hemo','pcv','rc']]],axis=1,sort=False)

"""## Fit a random forest classifier

Train a random forest classifier with 100 trees to estimate the probability of chronic kidney disease
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split


X_train, X_test, y_train, y_test = train_test_split(result, y, random_state=42,stratify=y, test_size = 0.25)

clf = RandomForestClassifier(n_estimators = 100,max_depth=20)
clf.fit(X_train, y_train)

y_pred=clf.predict(X_test)
y_pred

from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

"""## Plot the Feature Importances"""

feature_importance = pd.Series(clf.feature_importances_,index=result.columns).sort_values(ascending=False)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# Creating a bar plot
plt.figure(figsize=(11,7))
sns.barplot(x=feature_importance[0:35], y=feature_importance.index[0:35])

# Add labels to your graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.show()

"""## Explain individual predictions using SHAP (SHapley Additive exPlanations)

Goal: explain the predicted output of a data point by getting the contribution of each feature to the prediction.

Use TreeSHAP estimation method.
"""

explainer = shap.TreeExplainer(clf)

"""### Select four subjects from the test set, with different predicted outputs and analyze their explanations."""

y_test.loc[[0]]

print(clf.predict_proba(X_test.loc[[0]]))
clf.predict(X_test.loc[[0]])

subject1 = X_test.loc[[0]]
shap_values1 = explainer.shap_values(subject1)

shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values1[1], subject1)

"""Each feature value can increase (red) or decrease (blue) the prediction. The base value of 0.4391 represents the average of all predicted probabilites in the dataset.

Consider the first subject, that suffers from chronic kidney disease. The predicted risk is 0.61, where feature values such as the presence of Diabetes Mellitus (dm_yes=1) and Hypertension (htn_yes=1) increase the risk of kidney disease, but they are balanced out by the decreasing effects of Hemoglobin (hemo = 15.4), or Serum Creatinine (sc=1.2).
"""

print(y_test.loc[[271]])
print(clf.predict(X_test.loc[[271]]))
clf.predict_proba(X_test.loc[[271]])

subject2 = X_test.loc[[271]]
shap_values2 = explainer.shap_values(subject2)

shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values2[1], subject2)

"""The second subject has a risk of kidney disease of 0. Serum Creatinine of 0.5, Specific Gravity of 1.025, and a high value of Hemoglobin lower the probability of disease."""

print(clf.predict(X_test.loc[[25]]))
clf.predict_proba(X_test.loc[[25]])

subject3 = X_test.loc[[25]]
shap_values3 = explainer.shap_values(subject3)

shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values3[1], subject3)

"""This subject has a high risk of kidney disese of 0.96, where a lower value of Hemoglobin of 9.9 and a Serum Creatinine of 1.9 highly increase the probability of disease."""

print(clf.predict(X_test.loc[[15]]))
clf.predict_proba(X_test.loc[[15]])

subject4 = X_test.loc[[15]]
shap.initjs()
shap_values4 = explainer.shap_values(subject4)
shap.force_plot(explainer.expected_value[1], shap_values4[1], subject4)

"""For this subject, the probability of kidney disease of 1 is mostly increased by a low Hemoglobin of 7.6, a high Serum Creatinine of 9.6 and Specific Gravity of 1.015.

## SHAP Dependence Plots

A dependence plot represents the effect that a specific feature has on the predictions of the model.

### Dependence Plot for Hemoglobin
"""

shap_values = explainer.shap_values(X_test)
shp_plt = shap.dependence_plot("hemo", shap_values[1], X_test)

"""The above plot shows that for higher values of Hemoglobin ( > 13 ), the risk of kidney disease is constantly lower, whereas for values under 13 the probability of disease is much higher. It also shows that Hemoglobin interacts with Age frequently.

### Dependence Plot for Age
"""

shp_plt = shap.dependence_plot("age", shap_values[1], X_test)

"""This plot shows that Age is not relevant for the target variable until it reaches values higher than 60, where it increases the probability of disease. Also it can be noted that Age interacts with Blood Urea("bu") frequently.

### Feature Importances Plot
"""

shap.summary_plot(shap_values, X_test)